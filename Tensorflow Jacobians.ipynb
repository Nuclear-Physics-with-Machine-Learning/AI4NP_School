{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll look at Tensorflow's tools for computing a jacobian in the context of an SR algorithm.\n",
    "\n",
    "We'll initialize some input, a network, and try to compute the jacobian in several different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time, timeit\n",
    "\n",
    "N_WALKERS = 100\n",
    "DIM = 3\n",
    "N_PARTICLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.random.uniform(shape=(N_WALKERS, N_PARTICLES, DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetsWavefunction(tf.keras.models.Model):\n",
    "    \"\"\"Create a neural network eave function in N dimensions\n",
    "\n",
    "    Boundary condition, if not supplied, is gaussian in every dimension\n",
    "\n",
    "    Extends:\n",
    "        tf.keras.models.Model\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim : int, nparticles: int, mean_subtract : bool, boundary_condition :tf.keras.layers.Layer = None):\n",
    "        '''Deep Sets wavefunction for symmetric particle wavefunctions\n",
    "\n",
    "        Implements a deep set network for multiple particles in the same system\n",
    "\n",
    "        Arguments:\n",
    "            ndim {int} -- Number of dimensions\n",
    "            nparticles {int} -- Number of particls\n",
    "\n",
    "        Keyword Arguments:\n",
    "            boundary_condition {tf.keras.layers.Layer} -- [description] (default: {None})\n",
    "\n",
    "        Raises:\n",
    "            Exception -- [description]\n",
    "        '''\n",
    "        tf.keras.models.Model.__init__(self)\n",
    "\n",
    "        self.ndim = ndim\n",
    "        if self.ndim < 1 or self.ndim > 3:\n",
    "           raise Exception(\"Dimension must be 1, 2, or 3 for DeepSetsWavefunction\")\n",
    "\n",
    "        self.nparticles = nparticles\n",
    "\n",
    "        self.mean_subtract = mean_subtract\n",
    "\n",
    "\n",
    "        n_filters_per_layer = 8\n",
    "        n_layers            = 1\n",
    "        bias                = True\n",
    "        activation          = tf.keras.activations.tanh\n",
    "\n",
    "\n",
    "        self.individual_net = tf.keras.models.Sequential()\n",
    "\n",
    "        self.individual_net.add(\n",
    "            tf.keras.layers.Dense(n_filters_per_layer,\n",
    "                use_bias = bias)\n",
    "            )\n",
    "\n",
    "        for l in range(n_layers):\n",
    "            self.individual_net.add(\n",
    "                tf.keras.layers.Dense(n_filters_per_layer,\n",
    "                    use_bias    = bias,\n",
    "                    activation = activation)\n",
    "                )\n",
    "\n",
    "\n",
    "        self.aggregate_net = tf.keras.models.Sequential()\n",
    "\n",
    "        for l in range(n_layers):\n",
    "            self.individual_net.add(\n",
    "                tf.keras.layers.Dense(n_filters_per_layer,\n",
    "                    use_bias    = bias,\n",
    "                    activation = activation)\n",
    "                )\n",
    "        self.aggregate_net.add(tf.keras.layers.Dense(1,\n",
    "            use_bias = False))\n",
    "\n",
    "\n",
    "    @tf.function(experimental_compile=False)\n",
    "    def call(self, inputs, trainable=None):\n",
    "        # Mean subtract for all particles:\n",
    "        if self.nparticles > 1 and self.mean_subtract:\n",
    "            mean = tf.reduce_mean(inputs, axis=1)\n",
    "            xinputs = inputs - mean[:,None,:]\n",
    "        else:\n",
    "            xinputs = inputs\n",
    "\n",
    "        x = []\n",
    "        for p in range(self.nparticles):\n",
    "            x.append(self.individual_net(xinputs[:,p,:]))\n",
    "\n",
    "        x = tf.add_n(x)\n",
    "        x = self.aggregate_net(x)\n",
    "\n",
    "        # Compute the initial boundary condition, which the network will slowly overcome\n",
    "        # boundary_condition = tf.math.abs(self.normalization_weight * tf.reduce_sum(xinputs**self.normalization_exponent, axis=(1,2))\n",
    "        boundary_condition = -1. * tf.reduce_sum(xinputs**2, axis=(1,2))\n",
    "        boundary_condition = tf.reshape(boundary_condition, [-1,1])\n",
    "\n",
    "\n",
    "        return x + boundary_condition\n",
    "\n",
    "    def n_parameters(self):\n",
    "        return tf.reduce_sum( [ tf.reduce_prod(p.shape) for p in self.trainable_variables ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavefunction = DeepSetsWavefunction(ndim=DIM, nparticles=N_PARTICLES, mean_subtract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wavefunction(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a compiled wavfunction with a number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deep_sets_wavefunction\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (100, 8)                  176       \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (100, 1)                  8         \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(wavefunction.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "# In general, we can compute the gradient with respect to the inputs:\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    log_psiw = wavefunction(x_input)\n",
    "\n",
    "# By default, this essentially SUMS over the dimension of log_psiw\n",
    "print(log_psiw.shape)\n",
    "grads = tape.gradient(log_psiw, wavefunction.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n",
      "(8,)\n",
      "(8, 8)\n",
      "(8,)\n",
      "(8, 8)\n",
      "(8,)\n",
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "for g in grads:\n",
    "    print(g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the jacobian, instead of the gradient, which in this case is the gradient but for only one walker at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    log_psiw = wavefunction(x_input)\n",
    "\n",
    "# By default, this essentially SUMS over the dimension of log_psiw\n",
    "print(log_psiw.shape)\n",
    "jac = tape.jacobian(log_psiw, wavefunction.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 3, 8)\n",
      "(100, 1, 8)\n",
      "(100, 1, 8, 8)\n",
      "(100, 1, 8)\n",
      "(100, 1, 8, 8)\n",
      "(100, 1, 8)\n",
      "(100, 1, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "for j in jac:\n",
    "    print(j.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify this, when average, matches the total gradient: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.1956387e-07, shape=(), dtype=float32)\n",
      "tf.Tensor(3.0517578e-05, shape=(), dtype=float32)\n",
      "tf.Tensor(8.6426735e-07, shape=(), dtype=float32)\n",
      "tf.Tensor(4.5776367e-05, shape=(), dtype=float32)\n",
      "tf.Tensor(8.34465e-07, shape=(), dtype=float32)\n",
      "tf.Tensor(7.6293945e-05, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for j, g in zip(jac, grads):\n",
    "#     print(j)\n",
    "#     print(g)\n",
    "#     print(tf.reduce_sum(j, axis=(0,1)))\n",
    "    diff = tf.abs(g - tf.reduce_sum(j, axis=(0,1)))\n",
    "    print(tf.reduce_max(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every layer, the jacobian in this case is quite well matching the gradients if you sum over the input dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we should be able to compute this per walker too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    log_psiw = wavefunction(x_input)\n",
    "\n",
    "    split = tf.split(log_psiw, (1, N_WALKERS-1))\n",
    "    \n",
    "# print(split)\n",
    "# By default, this essentially SUMS over the dimension of log_psiw\n",
    "grad = tape.gradient(split[1], wavefunction.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def jacobian_comp(inputs, _wavefunction):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        log_psiw = _wavefunction(inputs)\n",
    "\n",
    "    # By default, this essentially SUMS over the dimension of log_psiw\n",
    "    jac = tape.jacobian(log_psiw, _wavefunction.trainable_variables)\n",
    "\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def jacobian_grad(inputs, _wavefunction):\n",
    "    \n",
    "    n_walkers = inputs.shape[0]\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        log_psiw = _wavefunction(inputs)\n",
    "\n",
    "        split = tf.split(log_psiw, n_walkers)\n",
    "\n",
    "    # print(split)\n",
    "    # By default, this essentially SUMS over the dimension of log_psiw\n",
    "    grad = [tape.gradient(s, _wavefunction.trainable_variables) for s in split]\n",
    "\n",
    "    jac = []\n",
    "    for i, l in enumerate(_wavefunction.trainable_variables):\n",
    "        temp = tf.stack([g[i] for g in grad])\n",
    "        temp = tf.reshape(temp,  log_psiw.shape + l.shape)\n",
    "        jac.append(temp)\n",
    "    \n",
    "    return jac\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation time:  1.5968520641326904\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "jc = jacobian_comp(x_input, wavefunction)\n",
    "print(\"Compilation time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation time:  13.892029047012329\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "jg = jacobian_grad(x_input, wavefunction)\n",
    "print(\"Compilation time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 ms ± 41.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jacobian_comp(x_input, wavefunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.82 ms ± 196 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jacobian_grad(x_input, wavefunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 3, 8)\n",
      "(100, 1, 3, 8)\n"
     ]
    }
   ],
   "source": [
    "print(jg[0].shape) \n",
    "print(jc[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN the end, the split version gives correct results and comparative performance (on my CPU) but has a dramatic compile time.  The main reason to pursue it is the reduced memory usage, which doesn't scale as N_WALKERS^2.  Tensorflow also has the option for limiting the number of parallel iterations in the jacobian calculation, which saves memory significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
